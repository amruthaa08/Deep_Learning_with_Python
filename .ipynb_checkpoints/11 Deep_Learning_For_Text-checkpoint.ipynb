{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b7aefb",
   "metadata": {},
   "source": [
    "# Steps in Text Preprocessing:\n",
    "    1. Standardizing text\n",
    "    2. Tokenizing text\n",
    "    3. Indexing (convert into numerical vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647fb4d",
   "metadata": {},
   "source": [
    "# 11.2.3 Vocabulary Indexing\n",
    "\n",
    "This is the step after standardising. It aims to build an index of all terms found in the data AKA **Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c1d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ()\n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this into a vector encoding:\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93630cff",
   "metadata": {},
   "source": [
    "# 11.2.4 Using the TextVectorization Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166538ae",
   "metadata": {},
   "source": [
    "## (i) Implementing from scratch usng pure python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea85ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "    \n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "        self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea3b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a vocabulary\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "\"I write, erase, rewrite\",\n",
    "\"Erase again, and then\",\n",
    "\"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdeca84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# encoding and decoding sample text\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566e561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da10f06",
   "metadata": {},
   "source": [
    "## (ii) Using a TextVectorization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1501a88",
   "metadata": {},
   "source": [
    "### Implementing textvectorization layer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36348c7f",
   "metadata": {},
   "source": [
    "### Using the keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42000456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "output_mode = \"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6740721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to index the vocabulary of a text corpus\n",
    "# use the adapt method\n",
    "\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms\"\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70834506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve the vocabulary using get_vocabulary() method\n",
    "# use this to decode sentences\n",
    "text_vectorization.get_vocabulary()\n",
    "# sorted by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c669ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "# encoding and decoding a sample sentence\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a994f54",
   "metadata": {},
   "source": [
    "# 11.3.1 Preparing the IMDB movie reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a5bf551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  2790k      0  0:00:29  0:00:29 --:--:-- 3883k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90379aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing an unrequired subdirectory\n",
    "!rm -r data/aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a8dd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "# seeing one of the samples\n",
    "!cat data/aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce964e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a validation set\n",
    "# using 20% of training data\n",
    "\n",
    "import os, pathlib, shutil, random\n",
    "base_dir = pathlib.Path(\"data/aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14cbdecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create dataset objects\n",
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "\"data/aclImdb/train/\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "\"data/aclImdb/val/\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "\"data/aclImdb/test/\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea32ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (32,)\n",
      "Inputs dtype:  <dtype: 'string'>\n",
      "Targets shape:  (32,)\n",
      "Targets dtype:  <dtype: 'int32'>\n",
      "Inputs[0]:  tf.Tensor(b\"Despite excellent trailers for Vanilla Sky, I was expecting to be disappointed by the film because I'd heard that it did not get great reviews. However, I left the cinema completely in awe of how good Vanilla Sky is.<br /><br />There was no bad acting at all in the whole film, every single character is believable. The romantic moments between Cruise's character, David Aames and Cruz's character, Sophia are tear-jerkingly realistic and intimate (probably due to the fact that they were a soon-to-be real-life couple).<br /><br />The plot of Vanilla Sky will confuse you in the last third of the film and there's very little chance of you guessing the ending. However, ends are tied up towards the end, leaving you with a strange mixture of feelings consisting of sadness, shock and empathy for David Aames.<br /><br />The film is intellectual and you have to pay attention throughout. This isn't that hard because chances are that you'll be completely drawn in to the film and won't take your eyes off the screen for one second.<br /><br />I usually leave cinemas forgetting all about the film I just watch. But Vanilla Sky is still lingering in my mind days after watching it. I recommend it to anyone who wants a change from simple, shallow films.\", shape=(), dtype=string)\n",
      "Targets[0]:  tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# displaying shapes and dtypes for 1st batch\n",
    "for inputs, targets in train_ds:\n",
    "    print(\"Input shape: \", inputs.shape)\n",
    "    print(\"Inputs dtype: \", inputs.dtype)\n",
    "    print(\"Targets shape: \", targets.shape)\n",
    "    print(\"Targets dtype: \", targets.dtype)\n",
    "    print(\"Inputs[0]: \", inputs[0])\n",
    "    print(\"Targets[0]: \", targets[0])    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27946b4",
   "metadata": {},
   "source": [
    "# 11.3.2 Processing words as a set: Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bd4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode raw data\n",
    "text_vectorization = keras.layers.TextVectorization(\n",
    "max_tokens=20000, output_mode=\"multi_hot\",)\n",
    "\n",
    "# create vocabulary by adapting to train inputs\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# prepare preprocessed versions of datasets\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting shapes and dtypes\n",
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"Input shape: \", inputs.shape)\n",
    "    print(\"Inputs dtype: \", inputs.dtype)\n",
    "    print(\"Targets shape: \", targets.shape)\n",
    "    print(\"Targets dtype: \", targets.dtype)\n",
    "    print(\"Inputs[0]: \", inputs[0])\n",
    "    print(\"Targets[0]: \", targets[0])    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e654a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reuable model-building function \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens = 20000, hidden_dims = 16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dims, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00def90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 55s 87ms/step - loss: 0.4175 - accuracy: 0.8176 - val_loss: 0.2761 - val_accuracy: 0.8956\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 25s 39ms/step - loss: 0.2776 - accuracy: 0.8982 - val_loss: 0.2732 - val_accuracy: 0.8980\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.2437 - accuracy: 0.9133 - val_loss: 0.2808 - val_accuracy: 0.9012\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 25s 41ms/step - loss: 0.2305 - accuracy: 0.9215 - val_loss: 0.2956 - val_accuracy: 0.9038\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.2230 - accuracy: 0.9266 - val_loss: 0.3116 - val_accuracy: 0.9008\n",
      "782/782 [==============================] - 65s 82ms/step - loss: 0.2939 - accuracy: 0.8843\n",
      "Test acc: 0.884\n"
     ]
    }
   ],
   "source": [
    "# train and test model\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/binary_1gram.keras\",\n",
    "                                           save_best_only=True)\n",
    "]\n",
    "\n",
    "# call dataset object with cache \n",
    "# only do preprocessing once and store it in memory\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "         validation_data = binary_1gram_val_ds.cache(),\n",
    "         epochs=5,\n",
    "         callbacks=callbacks)\n",
    "model = keras.models.load_model(\"models/binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9d901",
   "metadata": {},
   "source": [
    "## Bigrams with binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0cbbb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = layers.TextVectorization(\n",
    "    ngrams = 2,\n",
    "    max_tokens = 20000,\n",
    "    output_mode = \"multi_hot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16458822",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0da3251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 28s 43ms/step - loss: 0.3714 - accuracy: 0.8455 - val_loss: 0.2494 - val_accuracy: 0.9072\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.2340 - accuracy: 0.9134 - val_loss: 0.2504 - val_accuracy: 0.9108\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.2027 - accuracy: 0.9337 - val_loss: 0.2671 - val_accuracy: 0.9102\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 25s 39ms/step - loss: 0.1902 - accuracy: 0.9394 - val_loss: 0.2822 - val_accuracy: 0.9090\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 25s 40ms/step - loss: 0.1754 - accuracy: 0.9463 - val_loss: 0.3047 - val_accuracy: 0.9066\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.2636 - accuracy: 0.9001\n",
      "Test acc: 0.900\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/binary_2gram.keras\",\n",
    "                                           save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "         validation_data = binary_2gram_val_ds.cache(),\n",
    "         epochs=5,\n",
    "         callbacks=callbacks)\n",
    "model = keras.models.load_model(\"models/binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ee9ce",
   "metadata": {},
   "source": [
    "## Bigrams with TF-IDF Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09fe2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = layers.TextVectorization(\n",
    "    ngrams = 2,\n",
    "    max_tokens = 20000,\n",
    "    output_mode=\"tf_idf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e7888ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.5457 - accuracy: 0.7654 - val_loss: 0.2971 - val_accuracy: 0.8914\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.3608 - accuracy: 0.8512 - val_loss: 0.3132 - val_accuracy: 0.8752\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 24s 38ms/step - loss: 0.3227 - accuracy: 0.8644 - val_loss: 0.3099 - val_accuracy: 0.8878\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.2948 - accuracy: 0.8748 - val_loss: 0.3105 - val_accuracy: 0.8908\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.2849 - accuracy: 0.8748 - val_loss: 0.3289 - val_accuracy: 0.8768\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.3173 - accuracy: 0.8831\n",
      "Test acc: 0.883\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/tfidf_2gram.keras\",\n",
    "                                           save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "         validation_data = tfidf_2gram_val_ds.cache(),\n",
    "         epochs=5,\n",
    "         callbacks=callbacks)\n",
    "model = keras.models.load_model(\"models/tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f5add",
   "metadata": {},
   "source": [
    "# 11.3.3 The sequence model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5123287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600  # truncate sequences to 600 length\n",
    "max_tokens = 20000  # max size of vocabulary\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4\n",
    ")\n",
    "\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4\n",
    ")\n",
    "\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4\n",
    ")\n",
    "\n",
    "# data is now in the form of integer sequences\n",
    "# these need to be converted to one-hot-encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cee646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting shapes and dtypes\n",
    "for inputs, targets in int_train_ds:\n",
    "    print(\"Input shape: \", inputs.shape)\n",
    "    print(\"Inputs dtype: \", inputs.dtype)\n",
    "    print(\"Targets shape: \", targets.shape)\n",
    "    print(\"Targets dtype: \", targets.dtype)\n",
    "    print(\"Inputs[0]: \", inputs[0])\n",
    "    print(\"Targets[0]: \", targets[0])    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c09cfad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 600)]             0         \n",
      "                                                                 \n",
      " tf.one_hot (TFOpLambda)     (None, 600, 20000)        0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,128,513\n",
      "Trainable params: 5,128,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "import tensorflow as tf\n",
    "inputs = keras.Input(shape=(600,), dtype=\"int64\")  # sequence of integers\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)  # one hot encoding\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! DONT RUN !!\n",
    "# training model\n",
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\"models/one_hot_bidir_lstm.keras\",\n",
    "save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
    "callbacks=callbacks)\n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d875cf7",
   "metadata": {},
   "source": [
    "# Using Word Embeddings\n",
    "When using one-hot-encoding, you're providing an assumption to the model that individual tokens are independent from each other. One-hot-encodings are all orthogonal to each other. \n",
    "\n",
    "Hence **geometric relationship must reflect semantic relationship**.\n",
    "\n",
    "Word embeddings create a structured geometric space of human language - low dimensional dense floating-point vectors\n",
    "\n",
    "2 ways to create word embeddings:\n",
    "1. Jointly learn embeddings during training\n",
    "2. Use pretrained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39609989",
   "metadata": {},
   "source": [
    "# (i) Learning word embeddings with the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dfe454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: instantiating an embedding layer\n",
    "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdbfec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 600)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 600, 256)          5120000   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               73984     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model using embedding layer\n",
    "inputs = keras.Input(shape=(600,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8661553",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/embeddings_bidir_gru.keras\",\n",
    "                                           save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds,\n",
    "         validation_data = int_val_ds,\n",
    "         epochs=5,\n",
    "         callbacks=callbacks)\n",
    "model = keras.models.load_model(\"models/embeddings_bidir_gru.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f81d10",
   "metadata": {},
   "source": [
    "## Understanding padding and masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4583dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 7), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True,  True, False, False],\n",
       "       [ True,  True, False, False, False, False, False]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating an embedding layer with masking\n",
    "embedding_layer = layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\n",
    "some_input = [\n",
    "    [4, 3, 2, 1, 0, 0, 0],\n",
    "    [5, 4, 3, 2, 1, 0, 0],\n",
    "    [2, 1, 0, 0, 0, 0, 0]\n",
    "]\n",
    "# getting the computed mask\n",
    "mask = embedding_layer.compute_mask(some_input)\n",
    "mask\n",
    "# zeros are skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd8c00",
   "metadata": {},
   "source": [
    "## Modelling with masking used in embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ada541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               73984     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3738c91",
   "metadata": {},
   "source": [
    "# (ii) Using pretrained word embeddings\n",
    "\n",
    "examples: Word2Vec, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e916e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-09 19:16:16--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-04-09 19:16:16--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-04-09 19:16:18--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  3.46MB/s    in 5m 7s   \n",
      "\n",
      "2023-04-09 19:21:27 (2.67 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download GloVe word embeddings\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac53e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the unzipped file to build an index\n",
    "import numpy as np\n",
    "path_to_glove_file = \"data/glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e20bb8",
   "metadata": {},
   "source": [
    "### Did not do the rest, see book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4236058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2_kernel",
   "language": "python",
   "name": "env2_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
