{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b7aefb",
   "metadata": {},
   "source": [
    "# Steps in Text Preprocessing:\n",
    "    1. Standardizing text\n",
    "    2. Tokenizing text\n",
    "    3. Indexing (convert into numerical vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647fb4d",
   "metadata": {},
   "source": [
    "# 11.2.3 Vocabulary Indexing\n",
    "\n",
    "This is the step after standardising. It aims to build an index of all terms found in the data AKA **Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c1d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ()\n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this into a vector encoding:\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93630cff",
   "metadata": {},
   "source": [
    "# 11.2.4 Using the TextVectorization Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166538ae",
   "metadata": {},
   "source": [
    "## (i) Implementing from scratch usng pure python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea85ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "    \n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "        self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea3b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a vocabulary\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "\"I write, erase, rewrite\",\n",
    "\"Erase again, and then\",\n",
    "\"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdeca84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# encoding and decoding sample text\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566e561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da10f06",
   "metadata": {},
   "source": [
    "## (ii) Using a TextVectorization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1501a88",
   "metadata": {},
   "source": [
    "### Implementing textvectorization layer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36348c7f",
   "metadata": {},
   "source": [
    "### Using the keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42000456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "output_mode = \"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6740721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to index the vocabulary of a text corpus\n",
    "# use the adapt method\\\n",
    "\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms\"\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70834506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve the vocabulary using get_vocabulary() method\n",
    "# use this to decode sentences\n",
    "text_vectorization.get_vocabulary()\n",
    "# sorted by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c669ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "# encoding and decoding a sample sentence\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a994f54",
   "metadata": {},
   "source": [
    "# 11.3.1 Preparing the IMDB movie reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a5bf551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  2790k      0  0:00:29  0:00:29 --:--:-- 3883k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90379aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing an unrequired subdirectory\n",
    "!rm -r data/aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a8dd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "# seeing one of the samples\n",
    "!cat data/aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce964e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a validation set\n",
    "# using 20% of training data\n",
    "\n",
    "import os, pathlib, shutil, random\n",
    "base_dir = pathlib.Path(\"data/aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e8747ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 13:09:51.626944: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-09 13:09:51.636359: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create dataset objects\n",
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "\"data/aclImdb/train/\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "\"data/aclImdb/val/\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "\"data/aclImdb/test/\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e039bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (32,)\n",
      "Inputs dtype:  <dtype: 'string'>\n",
      "Targets shape:  (32,)\n",
      "Targets dtype:  <dtype: 'int32'>\n",
      "Inputs[0]:  tf.Tensor(b'The team of Merian Cooper and Ernest Schoedsack produced a documentary of 50,000 Bakhtiari people and their animals on the Summer migration to winter grazing. The basic worth of this film today is as a time capsule of a \"forgotten people\" and how they lived during what we in the West knew as the \"roaring twenties.\" A more drastic contrast could not be imagined. Raging river and barefoot mountain crossings are brutally realistic and the animals that disappear under the water do in fact die. To make sure that the audience of the time believed that the story took place, a signed certificate of authenticity is offered up at the end. The version that I saw had fascinating Iranian music that can stand alone and be appreciated without the film. Having said all this, the film is probably of more value to the anthropologist than the casual viewer in search of a good evening\\'s entertainment. The crew had just barely sufficient stock to take the shots that they recorded and there is no fancy camera work resulting from multiple re-takes. The Western inter-titles detract from the experience but are in fact a part of the record since they demonstrate how Hollywood tried to put their spin on the lives of an indigenous peoples lives so that they would be appreciated by the audience of the day. Off-duty entertainment by desert police becomes a \"policeman\\'s ball.\" The producers went on to make the docu-drama Chang (1927) and the totally commercial King Kong (1933). The migration theme is used again in People of the Wind (1976) and in Himalaya (1999). Recommended for those who know in advance what they are getting into -- and then highly recommended for them.', shape=(), dtype=string)\n",
      "Targets[0]:  tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# displaying shapes and dtypes for 1st batch\n",
    "for inputs, targets in train_ds:\n",
    "    print(\"Input shape: \", inputs.shape)\n",
    "    print(\"Inputs dtype: \", inputs.dtype)\n",
    "    print(\"Targets shape: \", targets.shape)\n",
    "    print(\"Targets dtype: \", targets.dtype)\n",
    "    print(\"Inputs[0]: \", inputs[0])\n",
    "    print(\"Targets[0]: \", targets[0])    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4cf3a",
   "metadata": {},
   "source": [
    "# 11.3.2 Processing words as a set: Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd188d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode raw data\n",
    "text_vectorization = keras.layers.TextVectorization(\n",
    "max_tokens=20000, output_mode=\"multi_hot\",)\n",
    "\n",
    "# create vocabulary by adapting to train inputs\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# prepare preprocessed versions of datasets\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e2d7be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (32, 20000)\n",
      "Inputs dtype:  <dtype: 'float32'>\n",
      "Targets shape:  (32,)\n",
      "Targets dtype:  <dtype: 'int32'>\n",
      "Inputs[0]:  tf.Tensor([1. 1. 0. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "Targets[0]:  tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# inspecting shapes and dtypes\n",
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"Input shape: \", inputs.shape)\n",
    "    print(\"Inputs dtype: \", inputs.dtype)\n",
    "    print(\"Targets shape: \", targets.shape)\n",
    "    print(\"Targets dtype: \", targets.dtype)\n",
    "    print(\"Inputs[0]: \", inputs[0])\n",
    "    print(\"Targets[0]: \", targets[0])    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeea39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reuable model-building function \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens = 20000, hidden_dims = 16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dims, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13345eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 55s 87ms/step - loss: 0.4175 - accuracy: 0.8176 - val_loss: 0.2761 - val_accuracy: 0.8956\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 25s 39ms/step - loss: 0.2776 - accuracy: 0.8982 - val_loss: 0.2732 - val_accuracy: 0.8980\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.2437 - accuracy: 0.9133 - val_loss: 0.2808 - val_accuracy: 0.9012\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 25s 41ms/step - loss: 0.2305 - accuracy: 0.9215 - val_loss: 0.2956 - val_accuracy: 0.9038\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.2230 - accuracy: 0.9266 - val_loss: 0.3116 - val_accuracy: 0.9008\n",
      "782/782 [==============================] - 65s 82ms/step - loss: 0.2939 - accuracy: 0.8843\n",
      "Test acc: 0.884\n"
     ]
    }
   ],
   "source": [
    "# train and test model\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/binary_1gram.keras\",\n",
    "                                           save_best_only=True)\n",
    "]\n",
    "\n",
    "# call dataset object with cache \n",
    "# only do preprocessing once and store it in memory\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "         validation_data = binary_1gram_val_ds.cache(),\n",
    "         epochs=5,\n",
    "         callbacks=callbacks)\n",
    "model = keras.models.load_model(\"models/binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60b2fe",
   "metadata": {},
   "source": [
    "## Bigrams with binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11231e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = layers.TextVectorization(\n",
    "    ngrams = 2,\n",
    "    max_tokens = 20000,\n",
    "    output_mode = \"multi_hot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed8b42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2ba65ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 28s 43ms/step - loss: 0.3714 - accuracy: 0.8455 - val_loss: 0.2494 - val_accuracy: 0.9072\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.2340 - accuracy: 0.9134 - val_loss: 0.2504 - val_accuracy: 0.9108\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.2027 - accuracy: 0.9337 - val_loss: 0.2671 - val_accuracy: 0.9102\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 25s 39ms/step - loss: 0.1902 - accuracy: 0.9394 - val_loss: 0.2822 - val_accuracy: 0.9090\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 25s 40ms/step - loss: 0.1754 - accuracy: 0.9463 - val_loss: 0.3047 - val_accuracy: 0.9066\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.2636 - accuracy: 0.9001\n",
      "Test acc: 0.900\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/binary_2gram.keras\",\n",
    "                                           save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "         validation_data = binary_2gram_val_ds.cache(),\n",
    "         epochs=5,\n",
    "         callbacks=callbacks)\n",
    "model = keras.models.load_model(\"models/binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25192a7",
   "metadata": {},
   "source": [
    "## Bigrams with TF-IDF Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4d203a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = layers.TextVectorization(\n",
    "    ngrams = 2,\n",
    "    max_tokens = 20000,\n",
    "    output_mode=\"tf_idf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ccf2cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.5457 - accuracy: 0.7654 - val_loss: 0.2971 - val_accuracy: 0.8914\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.3608 - accuracy: 0.8512 - val_loss: 0.3132 - val_accuracy: 0.8752\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 24s 38ms/step - loss: 0.3227 - accuracy: 0.8644 - val_loss: 0.3099 - val_accuracy: 0.8878\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.2948 - accuracy: 0.8748 - val_loss: 0.3105 - val_accuracy: 0.8908\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.2849 - accuracy: 0.8748 - val_loss: 0.3289 - val_accuracy: 0.8768\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.3173 - accuracy: 0.8831\n",
      "Test acc: 0.883\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "lambda x, y : (text_vectorization(x), y),\n",
    "num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/tfidf_2gram.keras\",\n",
    "                                           save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "         validation_data = tfidf_2gram_val_ds.cache(),\n",
    "         epochs=5,\n",
    "         callbacks=callbacks)\n",
    "model = keras.models.load_model(\"models/tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc239ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2_kernel",
   "language": "python",
   "name": "env2_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
