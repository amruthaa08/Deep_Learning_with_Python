{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa96e2e",
   "metadata": {},
   "source": [
    "# 8. 1 Classifying Mnist using a Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c1c53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-18 22:44:16.955021: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9421077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-18 22:47:01.972168: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-18 22:47:01.988257: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# creating model (functional API)\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(filters = 32, kernel_size = 3, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = layers.Conv2D(filters = 64, kernel_size = 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = layers.Conv2D(filters = 128, kernel_size = 3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc9b55ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,202\n",
      "Trainable params: 104,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c53b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 43s 45ms/step - loss: 0.1588 - accuracy: 0.9509\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 39s 41ms/step - loss: 0.0450 - accuracy: 0.9866\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 38s 41ms/step - loss: 0.0297 - accuracy: 0.9909\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 39s 41ms/step - loss: 0.0233 - accuracy: 0.9927\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 38s 40ms/step - loss: 0.0182 - accuracy: 0.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0cc3779d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting model\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c314e8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0250 - accuracy: 0.9919\n",
      "Test accuracy: 0.992\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfe8d7",
   "metadata": {},
   "source": [
    "# 8.2 Training a covnet from scratch on a small dataset\n",
    "Only code for reference, don't run the code as it's too much for the CPU to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8570aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into required directories\n",
    "import os, shutil, pathlib\n",
    "original_dir = pathlib.Path(\"data/train\")\n",
    "new_base_dir = pathlib.Path(\"data/cats_vs_dogs_small\")\n",
    "\n",
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            shutil.copyfile(src=original_dir / fname, dst=dir / fname)\n",
    "\n",
    "make_subset(\"train\", start_index=0, end_index=1000)\n",
    "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "make_subset(\"test\", start_index=1500, end_index=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdbde38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(32, (3, 3,), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(2, 2)(x)\n",
    "x = layers.Conv2D(64, (3, 3,), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(2, 2)(x)\n",
    "x = layers.Conv2D(128, (3, 3,), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(2, 2)(x)\n",
    "x = layers.Conv2D(256, (3, 3,), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(2, 2)(x)\n",
    "x = layers.Conv2D(256, (3, 3,), activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0484186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
      "                                                                 \n",
      " rescaling_6 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 178, 178, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 89, 89, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 87, 87, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 43, 43, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 41, 41, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 20, 20, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 18, 18, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 9, 9, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 7, 7, 256)         590080    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 12545     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 991,041\n",
      "Trainable params: 991,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e155fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85fbbab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n",
      "Found 2000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "train_dataset = image_dataset_from_directory(\n",
    "                new_base_dir / \"train\",\n",
    "                image_size=(180, 180),\n",
    "                batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "                new_base_dir / \"validation\",\n",
    "                image_size=(180, 180),\n",
    "                batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "                new_base_dir / \"test\",\n",
    "                image_size=(180, 180),\n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb777fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run mahn\n",
    "callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=30,\n",
    "        validation_data=validation_dataset,\n",
    "        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing loss and accuracy plots\n",
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706726f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa601d3",
   "metadata": {},
   "source": [
    "## Adding data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cb3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2_kernel",
   "language": "python",
   "name": "env2_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
